{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /root/scikit_learn_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _california_housing_dataset:\n",
      "\n",
      "California Housing dataset\n",
      "--------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 20640\n",
      "\n",
      "    :Number of Attributes: 8 numeric, predictive attributes and the target\n",
      "\n",
      "    :Attribute Information:\n",
      "        - MedInc        median income in block\n",
      "        - HouseAge      median house age in block\n",
      "        - AveRooms      average number of rooms\n",
      "        - AveBedrms     average number of bedrooms\n",
      "        - Population    block population\n",
      "        - AveOccup      average house occupancy\n",
      "        - Latitude      house block latitude\n",
      "        - Longitude     house block longitude\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "This dataset was obtained from the StatLib repository.\n",
      "http://lib.stat.cmu.edu/datasets/\n",
      "\n",
      "The target variable is the median house value for California districts.\n",
      "\n",
      "This dataset was derived from the 1990 U.S. census, using one row per census\n",
      "block group. A block group is the smallest geographical unit for which the U.S.\n",
      "Census Bureau publishes sample data (a block group typically has a population\n",
      "of 600 to 3,000 people).\n",
      "\n",
      "It can be downloaded/loaded using the\n",
      ":func:`sklearn.datasets.fetch_california_housing` function.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
      "      Statistics and Probability Letters, 33 (1997) 291-297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# get data\n",
    "data = fetch_california_housing()\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "\n",
    "print(data['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cali_df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "cali_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = cali_df.corr()\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, square=True, linewidths=.5, center=0, vmax=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20640, 8), dtype('float64'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,X.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20640, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.astype('float32')\n",
    "y = y.astype('float32').reshape(-1,1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.from_numpy(X)\n",
    "targets = torch.from_numpy(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader\n",
    "\n",
    "We'll create a `TensorDataset`, which allows access to rows from `inputs` and `targets` as tuples, and provides standard APIs for working with many different types of datasets in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 8.3252e+00,  4.1000e+01,  6.9841e+00,  1.0238e+00,  3.2200e+02,\n",
       "           2.5556e+00,  3.7880e+01, -1.2223e+02],\n",
       "         [ 8.3014e+00,  2.1000e+01,  6.2381e+00,  9.7188e-01,  2.4010e+03,\n",
       "           2.1098e+00,  3.7860e+01, -1.2222e+02],\n",
       "         [ 7.2574e+00,  5.2000e+01,  8.2881e+00,  1.0734e+00,  4.9600e+02,\n",
       "           2.8023e+00,  3.7850e+01, -1.2224e+02]]),\n",
       " tensor([[4.5260],\n",
       "         [3.5850],\n",
       "         [3.5210]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define dataset\n",
    "train_ds = TensorDataset(inputs, targets)\n",
    "train_ds[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TensorDataset` allows us to access a small section of the training data using the array indexing notation (`[0:3]` in the above code). It returns a tuple (or pair), in which the first element contains the input variables for the selected rows, and the second contains the targets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also create a `DataLoader`, which can split the data into batches of a predefined size while training. It also provides other utilities like shuffling and random sampling of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data loader\n",
    "batch_size = 1000\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   6.0516,   29.0000,    6.9581,  ...,    2.1535,   33.6300,\n",
      "         -117.9500],\n",
      "        [   2.8750,   26.0000,    5.1698,  ...,    4.1777,   34.9600,\n",
      "         -120.4500],\n",
      "        [   7.2982,    4.0000,    7.1844,  ...,    4.7416,   37.3300,\n",
      "         -121.8100],\n",
      "        ...,\n",
      "        [   4.5260,   21.0000,    6.2857,  ...,    2.6454,   33.8000,\n",
      "         -118.2900],\n",
      "        [   2.2925,   44.0000,    4.6693,  ...,    2.1901,   39.7300,\n",
      "         -121.8200],\n",
      "        [   4.7083,   28.0000,    5.8000,  ...,    2.6964,   33.8800,\n",
      "         -118.0000]])\n",
      "tensor([[5.0000],\n",
      "        [1.2140],\n",
      "        [3.4300],\n",
      "        [1.4570],\n",
      "        [2.2120],\n",
      "        [1.0880],\n",
      "        [1.5000],\n",
      "        [1.9940],\n",
      "        [2.3960],\n",
      "        [1.5400],\n",
      "        [1.8540],\n",
      "        [0.8510],\n",
      "        [0.5910],\n",
      "        [2.4570],\n",
      "        [0.7270],\n",
      "        [1.7600],\n",
      "        [0.8070],\n",
      "        [2.3240],\n",
      "        [2.2410],\n",
      "        [1.3750],\n",
      "        [2.2500],\n",
      "        [1.2780],\n",
      "        [1.1560],\n",
      "        [2.3180],\n",
      "        [2.1640],\n",
      "        [1.8580],\n",
      "        [0.8720],\n",
      "        [1.0730],\n",
      "        [0.7000],\n",
      "        [2.1480],\n",
      "        [1.1150],\n",
      "        [0.5330],\n",
      "        [5.0000],\n",
      "        [1.1940],\n",
      "        [0.9160],\n",
      "        [1.7950],\n",
      "        [1.0630],\n",
      "        [2.4940],\n",
      "        [0.5170],\n",
      "        [5.0000],\n",
      "        [0.6750],\n",
      "        [3.2580],\n",
      "        [1.3670],\n",
      "        [1.0660],\n",
      "        [1.0920],\n",
      "        [0.6080],\n",
      "        [2.0340],\n",
      "        [1.8230],\n",
      "        [5.0000],\n",
      "        [3.8330],\n",
      "        [1.6330],\n",
      "        [3.5000],\n",
      "        [1.0830],\n",
      "        [1.0060],\n",
      "        [0.9750],\n",
      "        [1.9440],\n",
      "        [5.0000],\n",
      "        [1.4010],\n",
      "        [1.2650],\n",
      "        [3.4590],\n",
      "        [3.1980],\n",
      "        [0.7530],\n",
      "        [1.8830],\n",
      "        [3.5000],\n",
      "        [5.0000],\n",
      "        [2.3130],\n",
      "        [1.6930],\n",
      "        [5.0000],\n",
      "        [1.9170],\n",
      "        [3.2230],\n",
      "        [1.1010],\n",
      "        [2.6250],\n",
      "        [5.0000],\n",
      "        [0.6150],\n",
      "        [1.1400],\n",
      "        [1.3350],\n",
      "        [1.6290],\n",
      "        [0.9350],\n",
      "        [1.1460],\n",
      "        [1.2780],\n",
      "        [5.0000],\n",
      "        [2.9010],\n",
      "        [0.8630],\n",
      "        [0.5000],\n",
      "        [4.3750],\n",
      "        [1.9380],\n",
      "        [1.1090],\n",
      "        [1.0110],\n",
      "        [1.3080],\n",
      "        [2.5000],\n",
      "        [3.8390],\n",
      "        [0.7780],\n",
      "        [1.2920],\n",
      "        [0.8500],\n",
      "        [1.7750],\n",
      "        [0.9480],\n",
      "        [2.4860],\n",
      "        [0.8830],\n",
      "        [1.4180],\n",
      "        [0.9800],\n",
      "        [1.8130],\n",
      "        [1.6840],\n",
      "        [3.3190],\n",
      "        [1.3900],\n",
      "        [0.6960],\n",
      "        [2.9450],\n",
      "        [2.7410],\n",
      "        [1.5540],\n",
      "        [0.9050],\n",
      "        [1.9910],\n",
      "        [0.9400],\n",
      "        [2.8670],\n",
      "        [1.5790],\n",
      "        [1.8640],\n",
      "        [3.2380],\n",
      "        [1.8710],\n",
      "        [2.9230],\n",
      "        [1.2580],\n",
      "        [1.6790],\n",
      "        [0.6320],\n",
      "        [2.4580],\n",
      "        [2.2500],\n",
      "        [2.0650],\n",
      "        [1.1610],\n",
      "        [1.3970],\n",
      "        [2.7730],\n",
      "        [2.0330],\n",
      "        [1.5770],\n",
      "        [1.0830],\n",
      "        [5.0000],\n",
      "        [1.8370],\n",
      "        [0.8620],\n",
      "        [2.6590],\n",
      "        [1.9750],\n",
      "        [2.3800],\n",
      "        [2.5830],\n",
      "        [2.2940],\n",
      "        [1.8420],\n",
      "        [1.1110],\n",
      "        [1.9110],\n",
      "        [1.5820],\n",
      "        [1.7980],\n",
      "        [1.8390],\n",
      "        [1.0360],\n",
      "        [2.6130],\n",
      "        [3.3500],\n",
      "        [1.0160],\n",
      "        [0.3750],\n",
      "        [1.0300],\n",
      "        [1.2880],\n",
      "        [1.8210],\n",
      "        [0.5500],\n",
      "        [1.2270],\n",
      "        [1.5350],\n",
      "        [2.0230],\n",
      "        [1.4640],\n",
      "        [1.8130],\n",
      "        [1.9190],\n",
      "        [1.5430],\n",
      "        [2.1250],\n",
      "        [1.5550],\n",
      "        [0.7330],\n",
      "        [1.0000],\n",
      "        [4.6990],\n",
      "        [2.7730],\n",
      "        [2.2690],\n",
      "        [0.7040],\n",
      "        [1.7650],\n",
      "        [1.1250],\n",
      "        [1.4020],\n",
      "        [3.6270],\n",
      "        [5.0000],\n",
      "        [1.5800],\n",
      "        [1.4430],\n",
      "        [0.6460],\n",
      "        [2.1350],\n",
      "        [1.4380],\n",
      "        [1.2910],\n",
      "        [1.1250],\n",
      "        [2.3930],\n",
      "        [1.3750],\n",
      "        [1.7120],\n",
      "        [4.4360],\n",
      "        [0.8030],\n",
      "        [3.6250],\n",
      "        [1.5000],\n",
      "        [2.5750],\n",
      "        [0.4880],\n",
      "        [2.4510],\n",
      "        [2.6250],\n",
      "        [1.8010],\n",
      "        [1.2070],\n",
      "        [2.6380],\n",
      "        [0.4040],\n",
      "        [1.9930],\n",
      "        [2.2730],\n",
      "        [2.2970],\n",
      "        [5.0000],\n",
      "        [2.3980],\n",
      "        [0.9410],\n",
      "        [2.6930],\n",
      "        [1.7030],\n",
      "        [3.2230],\n",
      "        [1.5400],\n",
      "        [0.8110],\n",
      "        [1.6890],\n",
      "        [0.9140],\n",
      "        [0.6220],\n",
      "        [0.6860],\n",
      "        [2.3070],\n",
      "        [2.4850],\n",
      "        [1.2240],\n",
      "        [3.3750],\n",
      "        [3.0580],\n",
      "        [0.9530],\n",
      "        [2.2500],\n",
      "        [0.9970],\n",
      "        [2.3130],\n",
      "        [0.7220],\n",
      "        [2.0810],\n",
      "        [2.2900],\n",
      "        [1.4030],\n",
      "        [0.8330],\n",
      "        [2.7130],\n",
      "        [1.0000],\n",
      "        [0.6100],\n",
      "        [1.7190],\n",
      "        [3.3940],\n",
      "        [2.2590],\n",
      "        [4.8720],\n",
      "        [3.1250],\n",
      "        [0.6750],\n",
      "        [0.6750],\n",
      "        [1.6280],\n",
      "        [0.5610],\n",
      "        [2.6550],\n",
      "        [1.6080],\n",
      "        [0.9510],\n",
      "        [1.2500],\n",
      "        [1.7550],\n",
      "        [2.3490],\n",
      "        [2.1840],\n",
      "        [2.9920],\n",
      "        [3.5910],\n",
      "        [4.2610],\n",
      "        [3.5420],\n",
      "        [3.2160],\n",
      "        [1.5000],\n",
      "        [1.6290],\n",
      "        [4.4210],\n",
      "        [2.7400],\n",
      "        [0.6940],\n",
      "        [5.0000],\n",
      "        [1.3750],\n",
      "        [2.3350],\n",
      "        [2.3740],\n",
      "        [0.7320],\n",
      "        [2.1560],\n",
      "        [1.3750],\n",
      "        [0.7370],\n",
      "        [0.4750],\n",
      "        [3.3410],\n",
      "        [5.0000],\n",
      "        [1.3410],\n",
      "        [1.3920],\n",
      "        [1.5630],\n",
      "        [1.1530],\n",
      "        [0.5440],\n",
      "        [3.0000],\n",
      "        [3.9110],\n",
      "        [1.3750],\n",
      "        [2.4060],\n",
      "        [0.5470],\n",
      "        [1.4220],\n",
      "        [2.3890],\n",
      "        [0.9170],\n",
      "        [4.3280],\n",
      "        [4.6270],\n",
      "        [2.6330],\n",
      "        [1.9790],\n",
      "        [2.4640],\n",
      "        [1.3210],\n",
      "        [1.2290],\n",
      "        [1.0000],\n",
      "        [3.0760],\n",
      "        [1.5440],\n",
      "        [3.7430],\n",
      "        [1.4150],\n",
      "        [1.8750],\n",
      "        [1.4410],\n",
      "        [2.4810],\n",
      "        [1.8420],\n",
      "        [1.0590],\n",
      "        [1.6030],\n",
      "        [3.2660],\n",
      "        [0.6130],\n",
      "        [1.6180],\n",
      "        [0.9030],\n",
      "        [3.3810],\n",
      "        [3.0000],\n",
      "        [1.0240],\n",
      "        [2.7880],\n",
      "        [2.6790],\n",
      "        [2.4200],\n",
      "        [2.6320],\n",
      "        [1.1810],\n",
      "        [3.1310],\n",
      "        [2.9750],\n",
      "        [5.0000],\n",
      "        [1.5310],\n",
      "        [1.9950],\n",
      "        [0.6850],\n",
      "        [1.7600],\n",
      "        [0.5860],\n",
      "        [3.6560],\n",
      "        [1.8730],\n",
      "        [2.8750],\n",
      "        [2.8660],\n",
      "        [0.7040],\n",
      "        [1.7830],\n",
      "        [1.6380],\n",
      "        [4.2640],\n",
      "        [2.6670],\n",
      "        [0.9450],\n",
      "        [2.1790],\n",
      "        [1.4580],\n",
      "        [5.0000],\n",
      "        [5.0000],\n",
      "        [2.6590],\n",
      "        [1.4580],\n",
      "        [2.4380],\n",
      "        [2.3330],\n",
      "        [1.1940],\n",
      "        [5.0000],\n",
      "        [0.4900],\n",
      "        [1.2250],\n",
      "        [0.9530],\n",
      "        [0.7100],\n",
      "        [1.0210],\n",
      "        [1.7500],\n",
      "        [2.7350],\n",
      "        [1.2970],\n",
      "        [0.8750],\n",
      "        [0.8690],\n",
      "        [2.6880],\n",
      "        [2.8500],\n",
      "        [2.3980],\n",
      "        [2.6950],\n",
      "        [1.1880],\n",
      "        [0.7970],\n",
      "        [5.0000],\n",
      "        [0.6930],\n",
      "        [2.6560],\n",
      "        [0.6200],\n",
      "        [2.2950],\n",
      "        [3.2400],\n",
      "        [1.0000],\n",
      "        [0.6800],\n",
      "        [2.3040],\n",
      "        [0.7420],\n",
      "        [2.6210],\n",
      "        [2.7680],\n",
      "        [2.2320],\n",
      "        [0.8960],\n",
      "        [0.9380],\n",
      "        [0.6240],\n",
      "        [2.2880],\n",
      "        [1.7540],\n",
      "        [1.0060],\n",
      "        [2.2500],\n",
      "        [1.7190],\n",
      "        [1.8380],\n",
      "        [2.0160],\n",
      "        [3.8120],\n",
      "        [2.4920],\n",
      "        [2.3140],\n",
      "        [4.7350],\n",
      "        [0.8390],\n",
      "        [1.3330],\n",
      "        [4.0450],\n",
      "        [2.4670],\n",
      "        [2.7010],\n",
      "        [1.3790],\n",
      "        [4.3140],\n",
      "        [3.3910],\n",
      "        [5.0000],\n",
      "        [0.9760],\n",
      "        [3.4040],\n",
      "        [0.5810],\n",
      "        [1.5160],\n",
      "        [2.7370],\n",
      "        [1.7440],\n",
      "        [0.7000],\n",
      "        [1.2650],\n",
      "        [2.9660],\n",
      "        [1.3260],\n",
      "        [2.5210],\n",
      "        [2.6710],\n",
      "        [2.2310],\n",
      "        [3.0220],\n",
      "        [2.7500],\n",
      "        [1.0380],\n",
      "        [1.8290],\n",
      "        [5.0000],\n",
      "        [5.0000],\n",
      "        [1.5340],\n",
      "        [1.1040],\n",
      "        [1.2120],\n",
      "        [0.5260],\n",
      "        [2.1010],\n",
      "        [2.8170],\n",
      "        [2.8610],\n",
      "        [1.5040],\n",
      "        [0.5280],\n",
      "        [5.0000],\n",
      "        [1.3320],\n",
      "        [5.0000],\n",
      "        [2.3800],\n",
      "        [1.1690],\n",
      "        [2.2710],\n",
      "        [1.5260],\n",
      "        [1.3350],\n",
      "        [3.0630],\n",
      "        [1.6030],\n",
      "        [3.9180],\n",
      "        [2.6110],\n",
      "        [2.6390],\n",
      "        [2.1280],\n",
      "        [2.1500],\n",
      "        [0.9050],\n",
      "        [2.2080],\n",
      "        [1.0000],\n",
      "        [1.3940],\n",
      "        [3.6940],\n",
      "        [1.4060],\n",
      "        [1.5630],\n",
      "        [0.8750],\n",
      "        [2.0860],\n",
      "        [0.7320],\n",
      "        [1.1250],\n",
      "        [1.9850],\n",
      "        [2.2710],\n",
      "        [3.7350],\n",
      "        [1.4100],\n",
      "        [1.5110],\n",
      "        [1.9060],\n",
      "        [1.7000],\n",
      "        [2.7150],\n",
      "        [2.2500],\n",
      "        [4.3160],\n",
      "        [3.2810],\n",
      "        [1.5920],\n",
      "        [1.2880],\n",
      "        [2.7500],\n",
      "        [1.8750],\n",
      "        [0.9650],\n",
      "        [0.7150],\n",
      "        [1.0150],\n",
      "        [2.5180],\n",
      "        [1.9310],\n",
      "        [1.4920],\n",
      "        [0.3250],\n",
      "        [2.7500],\n",
      "        [3.1390],\n",
      "        [1.6070],\n",
      "        [0.4770],\n",
      "        [1.4520],\n",
      "        [1.1080],\n",
      "        [2.3750],\n",
      "        [5.0000],\n",
      "        [3.3720],\n",
      "        [0.5170],\n",
      "        [2.5000],\n",
      "        [1.9520],\n",
      "        [0.9920],\n",
      "        [1.6210],\n",
      "        [1.9050],\n",
      "        [4.1670],\n",
      "        [1.8210],\n",
      "        [1.1350],\n",
      "        [1.6930],\n",
      "        [0.4500],\n",
      "        [2.0840],\n",
      "        [1.6920],\n",
      "        [1.3410],\n",
      "        [2.0270],\n",
      "        [0.7970],\n",
      "        [2.3040],\n",
      "        [0.7690],\n",
      "        [1.3020],\n",
      "        [1.7900],\n",
      "        [1.9350],\n",
      "        [5.0000],\n",
      "        [1.7650],\n",
      "        [1.8160],\n",
      "        [4.0980],\n",
      "        [0.5170],\n",
      "        [3.5710],\n",
      "        [1.3600],\n",
      "        [0.9620],\n",
      "        [0.8660],\n",
      "        [1.3850],\n",
      "        [0.9440],\n",
      "        [2.8850],\n",
      "        [2.7010],\n",
      "        [5.0000],\n",
      "        [4.6290],\n",
      "        [1.5480],\n",
      "        [1.4250],\n",
      "        [0.5480],\n",
      "        [2.6710],\n",
      "        [4.6670],\n",
      "        [1.6100],\n",
      "        [2.3240],\n",
      "        [0.5380],\n",
      "        [0.7080],\n",
      "        [2.3940],\n",
      "        [1.8650],\n",
      "        [0.6570],\n",
      "        [1.5650],\n",
      "        [2.2390],\n",
      "        [0.6790],\n",
      "        [2.6250],\n",
      "        [2.1230],\n",
      "        [2.3990],\n",
      "        [1.9660],\n",
      "        [1.1620],\n",
      "        [1.2480],\n",
      "        [2.2920],\n",
      "        [0.9210],\n",
      "        [4.7780],\n",
      "        [0.7840],\n",
      "        [2.2310],\n",
      "        [1.2080],\n",
      "        [1.0300],\n",
      "        [1.1980],\n",
      "        [0.9860],\n",
      "        [1.9050],\n",
      "        [2.8780],\n",
      "        [3.2500],\n",
      "        [1.6250],\n",
      "        [0.9750],\n",
      "        [0.6560],\n",
      "        [1.3080],\n",
      "        [2.1650],\n",
      "        [1.3130],\n",
      "        [1.6250],\n",
      "        [2.1820],\n",
      "        [2.3800],\n",
      "        [2.1280],\n",
      "        [0.8580],\n",
      "        [1.4090],\n",
      "        [1.5860],\n",
      "        [3.0060],\n",
      "        [2.1590],\n",
      "        [3.4070],\n",
      "        [5.0000],\n",
      "        [1.1250],\n",
      "        [1.1680],\n",
      "        [2.3310],\n",
      "        [5.0000],\n",
      "        [0.9890],\n",
      "        [1.7660],\n",
      "        [2.7030],\n",
      "        [5.0000],\n",
      "        [1.8270],\n",
      "        [1.7680],\n",
      "        [1.1190],\n",
      "        [3.1250],\n",
      "        [1.6070],\n",
      "        [1.1630],\n",
      "        [2.1500],\n",
      "        [2.0210],\n",
      "        [4.3510],\n",
      "        [1.4060],\n",
      "        [0.7130],\n",
      "        [2.3980],\n",
      "        [1.6500],\n",
      "        [1.4030],\n",
      "        [3.6560],\n",
      "        [3.5390],\n",
      "        [1.5690],\n",
      "        [2.5310],\n",
      "        [4.8220],\n",
      "        [0.8830],\n",
      "        [1.1430],\n",
      "        [4.3570],\n",
      "        [1.6780],\n",
      "        [5.0000],\n",
      "        [1.0520],\n",
      "        [1.6250],\n",
      "        [3.8220],\n",
      "        [1.9340],\n",
      "        [2.6670],\n",
      "        [4.4450],\n",
      "        [2.1550],\n",
      "        [1.0150],\n",
      "        [2.3790],\n",
      "        [2.4340],\n",
      "        [3.2750],\n",
      "        [0.7730],\n",
      "        [2.6800],\n",
      "        [1.0830],\n",
      "        [0.4950],\n",
      "        [1.6170],\n",
      "        [1.2620],\n",
      "        [0.5300],\n",
      "        [5.0000],\n",
      "        [5.0000],\n",
      "        [1.3720],\n",
      "        [1.4690],\n",
      "        [1.7180],\n",
      "        [1.6820],\n",
      "        [0.6520],\n",
      "        [4.1300],\n",
      "        [2.0810],\n",
      "        [0.8890],\n",
      "        [5.0000],\n",
      "        [2.0650],\n",
      "        [2.1130],\n",
      "        [1.7860],\n",
      "        [3.6590],\n",
      "        [1.4260],\n",
      "        [1.1900],\n",
      "        [4.5920],\n",
      "        [2.0210],\n",
      "        [0.5500],\n",
      "        [3.3500],\n",
      "        [5.0000],\n",
      "        [1.8450],\n",
      "        [0.5500],\n",
      "        [0.8750],\n",
      "        [3.0030],\n",
      "        [2.4920],\n",
      "        [5.0000],\n",
      "        [0.7590],\n",
      "        [1.7290],\n",
      "        [2.0020],\n",
      "        [0.7380],\n",
      "        [2.2880],\n",
      "        [0.5440],\n",
      "        [1.5630],\n",
      "        [0.5920],\n",
      "        [1.1920],\n",
      "        [1.3700],\n",
      "        [2.5080],\n",
      "        [2.7220],\n",
      "        [1.6300],\n",
      "        [1.5970],\n",
      "        [0.8590],\n",
      "        [1.4550],\n",
      "        [1.2250],\n",
      "        [2.5510],\n",
      "        [1.0130],\n",
      "        [1.2500],\n",
      "        [3.0000],\n",
      "        [1.1250],\n",
      "        [0.8180],\n",
      "        [3.6730],\n",
      "        [3.4060],\n",
      "        [2.2690],\n",
      "        [2.0280],\n",
      "        [1.0420],\n",
      "        [2.7500],\n",
      "        [0.9340],\n",
      "        [1.5710],\n",
      "        [1.9960],\n",
      "        [0.6110],\n",
      "        [1.1410],\n",
      "        [3.2060],\n",
      "        [2.5380],\n",
      "        [1.6560],\n",
      "        [1.5930],\n",
      "        [1.4820],\n",
      "        [5.0000],\n",
      "        [2.0370],\n",
      "        [1.5850],\n",
      "        [1.7480],\n",
      "        [4.6380],\n",
      "        [3.0180],\n",
      "        [2.5220],\n",
      "        [1.3750],\n",
      "        [5.0000],\n",
      "        [2.2740],\n",
      "        [3.4580],\n",
      "        [2.7730],\n",
      "        [0.7840],\n",
      "        [5.0000],\n",
      "        [3.2880],\n",
      "        [2.5440],\n",
      "        [2.0080],\n",
      "        [2.9640],\n",
      "        [2.6640],\n",
      "        [3.6000],\n",
      "        [1.4380],\n",
      "        [2.5350],\n",
      "        [1.8440],\n",
      "        [4.7000],\n",
      "        [2.3750],\n",
      "        [2.5790],\n",
      "        [1.6560],\n",
      "        [0.7200],\n",
      "        [1.2060],\n",
      "        [1.5190],\n",
      "        [2.1050],\n",
      "        [0.9870],\n",
      "        [0.8470],\n",
      "        [2.6410],\n",
      "        [4.2550],\n",
      "        [4.0580],\n",
      "        [3.8180],\n",
      "        [2.3460],\n",
      "        [1.2930],\n",
      "        [2.2910],\n",
      "        [5.0000],\n",
      "        [3.3520],\n",
      "        [5.0000],\n",
      "        [4.9860],\n",
      "        [2.1950],\n",
      "        [3.9380],\n",
      "        [5.0000],\n",
      "        [2.0610],\n",
      "        [5.0000],\n",
      "        [1.9380],\n",
      "        [2.6210],\n",
      "        [2.3650],\n",
      "        [1.7250],\n",
      "        [0.6200],\n",
      "        [0.5340],\n",
      "        [2.2020],\n",
      "        [1.9800],\n",
      "        [4.4680],\n",
      "        [3.5870],\n",
      "        [0.8330],\n",
      "        [1.6950],\n",
      "        [2.0810],\n",
      "        [1.0270],\n",
      "        [3.5450],\n",
      "        [1.6900],\n",
      "        [1.3060],\n",
      "        [5.0000],\n",
      "        [1.8310],\n",
      "        [0.8510],\n",
      "        [2.2500],\n",
      "        [1.6570],\n",
      "        [1.6630],\n",
      "        [1.3880],\n",
      "        [1.8560],\n",
      "        [2.0740],\n",
      "        [1.5010],\n",
      "        [5.0000],\n",
      "        [0.7010],\n",
      "        [1.5590],\n",
      "        [2.4580],\n",
      "        [0.9870],\n",
      "        [0.9500],\n",
      "        [1.0630],\n",
      "        [1.5000],\n",
      "        [2.1670],\n",
      "        [0.5440],\n",
      "        [2.5460],\n",
      "        [5.0000],\n",
      "        [3.4400],\n",
      "        [2.6900],\n",
      "        [0.5500],\n",
      "        [2.5540],\n",
      "        [2.5980],\n",
      "        [1.8640],\n",
      "        [0.5060],\n",
      "        [3.1010],\n",
      "        [4.4170],\n",
      "        [1.1810],\n",
      "        [1.5920],\n",
      "        [0.6220],\n",
      "        [0.7380],\n",
      "        [1.0560],\n",
      "        [2.5840],\n",
      "        [1.0620],\n",
      "        [1.4850],\n",
      "        [2.3500],\n",
      "        [1.6860],\n",
      "        [3.2730],\n",
      "        [1.3410],\n",
      "        [2.0180],\n",
      "        [5.0000],\n",
      "        [1.2040],\n",
      "        [2.5460],\n",
      "        [1.2720],\n",
      "        [1.8450],\n",
      "        [0.5790],\n",
      "        [1.3530],\n",
      "        [2.6780],\n",
      "        [1.5380],\n",
      "        [2.5610],\n",
      "        [0.4920],\n",
      "        [1.7940],\n",
      "        [3.5590],\n",
      "        [2.9370],\n",
      "        [1.4830],\n",
      "        [2.3020],\n",
      "        [2.2810],\n",
      "        [1.9610],\n",
      "        [3.3730],\n",
      "        [0.6750],\n",
      "        [1.7750],\n",
      "        [1.0400],\n",
      "        [2.3460],\n",
      "        [1.9150],\n",
      "        [3.5000],\n",
      "        [2.8910],\n",
      "        [0.5790],\n",
      "        [1.5360],\n",
      "        [1.4200],\n",
      "        [1.1350],\n",
      "        [1.2350],\n",
      "        [1.1810],\n",
      "        [0.8930],\n",
      "        [1.5960],\n",
      "        [5.0000],\n",
      "        [2.1610],\n",
      "        [1.2720],\n",
      "        [1.1180],\n",
      "        [5.0000],\n",
      "        [1.8020],\n",
      "        [1.9840],\n",
      "        [3.5000],\n",
      "        [0.9270],\n",
      "        [2.0460],\n",
      "        [5.0000],\n",
      "        [0.9330],\n",
      "        [2.5230],\n",
      "        [2.6770],\n",
      "        [0.5170],\n",
      "        [2.2500],\n",
      "        [1.9220],\n",
      "        [1.1720],\n",
      "        [1.4500],\n",
      "        [0.5250],\n",
      "        [1.7750],\n",
      "        [1.2560],\n",
      "        [1.9670],\n",
      "        [1.9560],\n",
      "        [0.8130],\n",
      "        [0.6610],\n",
      "        [1.1510],\n",
      "        [4.6350],\n",
      "        [1.6480],\n",
      "        [1.1490],\n",
      "        [1.6650],\n",
      "        [2.5450],\n",
      "        [4.1430],\n",
      "        [1.7260],\n",
      "        [4.9550],\n",
      "        [2.7940],\n",
      "        [1.6960],\n",
      "        [0.5650],\n",
      "        [1.8750],\n",
      "        [0.9310],\n",
      "        [1.9060],\n",
      "        [1.7840],\n",
      "        [0.9980],\n",
      "        [0.9680],\n",
      "        [1.0910],\n",
      "        [0.7100],\n",
      "        [2.0240],\n",
      "        [0.6890],\n",
      "        [1.2140],\n",
      "        [1.1650],\n",
      "        [0.6970],\n",
      "        [0.8310],\n",
      "        [2.3020],\n",
      "        [1.3720],\n",
      "        [0.5500],\n",
      "        [1.7170],\n",
      "        [1.6250],\n",
      "        [2.5100],\n",
      "        [2.0940],\n",
      "        [2.4600],\n",
      "        [1.8510],\n",
      "        [3.2540],\n",
      "        [1.0360],\n",
      "        [5.0000],\n",
      "        [1.0670],\n",
      "        [0.9730],\n",
      "        [2.2980],\n",
      "        [1.9200],\n",
      "        [0.8170],\n",
      "        [2.1710],\n",
      "        [1.1900],\n",
      "        [2.3460],\n",
      "        [1.3750],\n",
      "        [0.7620],\n",
      "        [1.6940],\n",
      "        [1.8900],\n",
      "        [0.6750],\n",
      "        [3.5510],\n",
      "        [1.3250],\n",
      "        [0.5410],\n",
      "        [1.7570],\n",
      "        [1.3750],\n",
      "        [1.4480],\n",
      "        [3.3380],\n",
      "        [0.9760],\n",
      "        [2.2010],\n",
      "        [2.0180],\n",
      "        [1.9590],\n",
      "        [1.9590],\n",
      "        [1.4380],\n",
      "        [0.6730],\n",
      "        [1.3750],\n",
      "        [5.0000],\n",
      "        [1.5380],\n",
      "        [1.3190],\n",
      "        [2.9560],\n",
      "        [1.2300],\n",
      "        [2.7820],\n",
      "        [1.1900],\n",
      "        [1.7970],\n",
      "        [3.7820],\n",
      "        [1.6250],\n",
      "        [2.3140],\n",
      "        [0.8880],\n",
      "        [2.5940],\n",
      "        [1.1730],\n",
      "        [5.0000],\n",
      "        [4.5000],\n",
      "        [1.3570],\n",
      "        [4.6880],\n",
      "        [2.1810],\n",
      "        [1.6690],\n",
      "        [3.1100],\n",
      "        [0.8340],\n",
      "        [1.5600],\n",
      "        [0.8540],\n",
      "        [0.9750],\n",
      "        [0.5520],\n",
      "        [0.5670],\n",
      "        [0.8770],\n",
      "        [1.0150],\n",
      "        [3.4900],\n",
      "        [0.6100],\n",
      "        [1.6030],\n",
      "        [1.1610],\n",
      "        [0.9920],\n",
      "        [1.9180],\n",
      "        [1.5000],\n",
      "        [1.0980],\n",
      "        [3.7720],\n",
      "        [3.4120],\n",
      "        [2.3610],\n",
      "        [0.5600],\n",
      "        [0.5400],\n",
      "        [0.7220],\n",
      "        [1.0210],\n",
      "        [2.3150],\n",
      "        [0.9060],\n",
      "        [1.5500],\n",
      "        [1.7500],\n",
      "        [1.7380],\n",
      "        [1.2570],\n",
      "        [3.1670],\n",
      "        [3.6350],\n",
      "        [2.4670],\n",
      "        [3.3550],\n",
      "        [1.9740],\n",
      "        [1.1410],\n",
      "        [1.9070],\n",
      "        [1.0890],\n",
      "        [2.4870],\n",
      "        [1.8750],\n",
      "        [0.9550],\n",
      "        [4.1130],\n",
      "        [1.1540],\n",
      "        [2.9380],\n",
      "        [1.2920],\n",
      "        [2.2440],\n",
      "        [2.4470],\n",
      "        [1.5680],\n",
      "        [5.0000],\n",
      "        [0.9350],\n",
      "        [2.4370],\n",
      "        [1.3050],\n",
      "        [1.0000],\n",
      "        [1.1480],\n",
      "        [1.3330],\n",
      "        [2.0130],\n",
      "        [2.2270],\n",
      "        [4.8510],\n",
      "        [4.5000],\n",
      "        [1.4130],\n",
      "        [1.0210],\n",
      "        [2.1740],\n",
      "        [3.4550],\n",
      "        [0.7460],\n",
      "        [1.5490],\n",
      "        [2.3530],\n",
      "        [3.1800],\n",
      "        [3.2940],\n",
      "        [0.8580],\n",
      "        [2.6810]])\n"
     ]
    }
   ],
   "source": [
    "for xb, yb in train_dl:\n",
    "    print(xb)\n",
    "    print(yb)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.2546,  0.1365, -0.0084, -0.0081, -0.2817,  0.2600, -0.0805,  0.2543]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1031], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "model = nn.Linear(8, 1) #nn.Linear(no of input features,no of targets)\n",
    "print(model.weight)\n",
    "print(model.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch models also have a helpful .parameters method, which returns a list containing all the weights and bias matrices present in the model. For our linear regression model, we have one weight matrix and one bias matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.randn` creates a tensor with the given shape, with elements picked randomly from a [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution) with mean 0 and standard deviation 1.\n",
    "\n",
    "Our *model* is simply a function that performs a matrix multiplication of the `inputs` and the weights `w` (transposed) and adds the bias `b` (replicated for each observation).\n",
    "\n",
    "![matrix-mult](https://i.imgur.com/WGXLFvA.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of input features is 8 and the no of target features is 1.Therefore our weight matrix is (8,1).\n",
    "Since we are using linear regression and our data has eight features, our model is\n",
    "\n",
    "$$ y(X) = \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_4 + \\beta_5 x_5 + \\beta_6 x_6 + \\beta_7 x_7 + \\beta_8 x_8 + \\beta_0. $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust weights and biases using gradient descent\n",
    "\n",
    "We'll reduce the loss and improve our model using the gradient descent optimization algorithm, which has the following steps:\n",
    "\n",
    "1. Generate predictions\n",
    "\n",
    "2. Calculate the loss\n",
    "\n",
    "3. Compute gradients w.r.t the weights and biases\n",
    "\n",
    "4. Adjust the weights by subtracting a small quantity proportional to the gradient\n",
    "\n",
    "5. Reset the gradients to zero\n",
    "\n",
    "Let's implement the above step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import nn.functional\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "Instead of manually manipulating the model's weights & biases using gradients, we can use the optimizer `optim.SGD`. SGD stands for `stochastic gradient descent`. It is called `stochastic` because samples are selected in batches (often with random shuffling) instead of as a single group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "loss_fn = F.mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(289696.7812, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "loss = loss_fn(model(inputs), targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "opt = torch.optim.SGD(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "We are now ready to train the model. We'll follow the exact same process to implement gradient descent:\n",
    "\n",
    "1. Generate predictions\n",
    "\n",
    "2. Calculate the loss\n",
    "\n",
    "3. Compute gradients w.r.t the weights and biases\n",
    "\n",
    "4. Adjust the weights by subtracting a small quantity proportional to the gradient\n",
    "\n",
    "5. Reset the gradients to zero\n",
    "\n",
    "The only change is that we'll work batches of data, instead of processing the entire training data in every iteration. Let's define a utility function `fit` which trains the model for a given number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to train the model\n",
    "def fit(num_epochs, model, loss_fn, opt):\n",
    "    \n",
    "    # Repeat for given number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Train with batches of data\n",
    "        for xb,yb in train_dl:\n",
    "            \n",
    "            # 1. Generate predictions\n",
    "            pred = model(xb)\n",
    "            print(pred)\n",
    "            # 2. Calculate loss\n",
    "            loss = loss_fn(pred, yb)\n",
    "            print(loss)\n",
    "            # 3. Compute gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # 4. Update parameters using gradients\n",
    "            opt.step()\n",
    "            \n",
    "            # 5. Reset the gradients to zero\n",
    "            opt.zero_grad()\n",
    "        \n",
    "        # Print the progress\n",
    "        #for epoch in range(10):\n",
    "           # print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "            #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some things to note above:\n",
    "\n",
    "* We use the data loader defined earlier to get batches of data for every iteration.\n",
    "\n",
    "* Instead of updating parameters (weights and biases) manually, we use `opt.step` to perform the update, and `opt.zero_grad` to reset the gradients to zero.\n",
    "\n",
    "* We've also added a log statement which prints the loss from the last batch of data for every 10th epoch, to track the progress of training. `loss.item` returns the actual value stored in the loss tensor.\n",
    "\n",
    "Let's train the model for 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(20, model, loss_fn, opt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
